{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this program we predict stats using ngrams, Using feature, ngrams + 25 features seperately and all three models are predicted and applied on SVM, MLP and decision tree. So there are total 9 models. Program only take 30 seconds to predict fit all models without using any pickle form which shows that model is very very fast. \n",
    "\n",
    "\n",
    "FEATURES\n",
    "\n",
    "+ col1 Elongated words\n",
    "+ col2 number of hshtags (FOR BONUS)\n",
    "+ col3 number of capitals letter\n",
    "+ col4 number of tagged persons in a sentence (FOR BONUS)\n",
    "+ col5 for negations\n",
    "+ col6 count words having particular emotion(joy)\n",
    "+ col7 aggregate hashtag emotion value\n",
    "+ col8 Aggregate emotion score (Hashtags)\n",
    "+ col9  Emoticons score:\n",
    "+ col 10,11 Aggregate polarity scores:\n",
    "+ col 12 13 Aggregate polarity scores (Hashtags)\n",
    "+ col 14 15 16 17  Lexicon based Features:\n",
    "+ col 18 19 Punctuation feature (BONUS)\n",
    "+ col 20 negation feature (take whole phrase) (BONUS)\n",
    "+ col 21 22 23 24 Vader\n",
    "+ col25 Only hashtags predicting nature for sentence  (only for this training data) (FOR BONUS) \n",
    "\n",
    "\n",
    "\n",
    "++ NGRAM (1,2) Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from time import time\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from os import path\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn import metrics\n",
    "from sys import getsizeof\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPClassifier , MLPRegressor\n",
    "from scipy import stats\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "from sklearn.tree import DecisionTreeRegressor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict model where:\n",
    "regressor- Regressor used (SVC,MLP,Decision tree)\n",
    "traindata- training data \n",
    "testdata- testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelprid(regressor,traindata,testdata,trainres,testres): \n",
    "    start_time = time()\n",
    "    model = regressor\n",
    "    model.fit(traindata,trainres)\n",
    "    sss = model.predict(testdata)\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(rte, predic))\n",
    "    statis(testres,sss)\n",
    "\n",
    "    #print(\"\\nTime = \",time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the features used to predict model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlongated(st):        # For enlongated words\n",
    "\ttemp = re.sub(r'(.)\\1+', r'\\1\\1', st)\n",
    "\tif len(temp) == len(st):\n",
    "\t\treturn 0\n",
    "\telse:\n",
    "\t\treturn 1\n",
    "    \n",
    "def hashtag(st):         # to count hashtags\n",
    "    if st[0] == \"#\":\n",
    "        return 1\n",
    "    return 0 \n",
    "\n",
    "def allcaps(st):        # to count all capital letters\n",
    "    if st.isupper():\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def taggedp(st):      # to count tagged persons in the sentence\n",
    "    if st[0] == \"@\":\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def negations(st):    # to count negations\n",
    "    if st.lower() in [\"not\",\"nor\",\"neither\",\"no\",\"never\",\"nope\"]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def hashemo(st,dic):    # to count Aggregate emotion score\n",
    "    tem = []\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            tem.append(dic[i.lower()])\n",
    "    if tem == []:\n",
    "        return 0\n",
    "    return sum(tem)/len(tem)\n",
    "\n",
    "def wordemo(st,dic):         # Counts Emotion word count:\n",
    "    if st.lower() in dic:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def sentiment_scores(sentence):      # Used in vader\n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "    return sid_obj.polarity_scores(sentence)\n",
    "    #print(sentiment_dict)\n",
    "            \n",
    "def vader(score,trainall):   # Vader returns 4 feature (positivity, neutrality, negativity, compound)\n",
    "    for i in range(len(trainall)):\n",
    "        temp = sentiment_scores(trainall[i])\n",
    "        #print(temp)\n",
    "        score[i].append(temp[\"pos\"])\n",
    "        score[i].append(temp[\"neu\"])\n",
    "        score[i].append(temp[\"neg\"])\n",
    "        score[i].append(temp[\"compound\"])\n",
    "        \n",
    "def emotionsfeat(st,dic):     #Aggregate emotion score (Hashtags)\n",
    "    tem = []\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            tem.append(float(dic[i.lower()]))\n",
    "    if tem == []:\n",
    "        return 0\n",
    "    return sum(tem)/len(tem)\n",
    "\n",
    "def emoticonfeat(st,dic):  #  Emoticons score\n",
    "    aa = 0\n",
    "    for i in st:\n",
    "        if i in dic:\n",
    "            aa += dic[i]\n",
    "    return aa\n",
    "\n",
    "def sent140feat(st,dic):   #Aggregate polarity scores:\n",
    "    posi = []\n",
    "    neg = []\n",
    "    for i in st:\n",
    "        if i in dic:\n",
    "            posi.append(dic[i][0])\n",
    "            neg.append(dic[i][1])\n",
    "    if posi == []:\n",
    "        return 0,0\n",
    "    return sum(posi)/len(posi) , sum(neg)/len(neg)\n",
    "\n",
    "def hashsentfeat(st,dic):       #Aggregate polarity scores (Hashtags)\n",
    "    posi = []\n",
    "    neg = []\n",
    "    for i in st:\n",
    "        if i in dic:\n",
    "            posi.append(dic[i][0])\n",
    "            neg.append(dic[i][1])\n",
    "    if posi == []:\n",
    "        return 0,0\n",
    "    return sum(posi)/len(posi) , sum(neg)/len(neg)\n",
    "\n",
    "def mpqafeat(st,dic):            #Lexicon based Features\n",
    "    posi = 0\n",
    "    neg = 0\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            if dic[i.lower()] == \"positive\":\n",
    "                posi += 1\n",
    "            elif dic[i.lower()] == \"negative\":\n",
    "                neg += 1\n",
    "    return posi, neg\n",
    "\n",
    "def bingliufeat(st,dic):         #Lexicon based Features\n",
    "    posi = 0\n",
    "    neg = 0\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            #print(i.lower())\n",
    "            #print(dic[i.lower()])\n",
    "            if dic[i.lower()] == \"positive\":\n",
    "                posi += 1\n",
    "            elif dic[i.lower()] == \"negative\":\n",
    "                neg += 1\n",
    "    return posi, neg\n",
    "\n",
    "def puncfeat(i):              # Punctuation Feature\n",
    "    if i[-1] == \"!\" or i[-1]==\"?\":\n",
    "        return len(re.findall(\"[!?]{2,}\",i)) , 1\n",
    "    else:\n",
    "        return len(re.findall(\"[!?]{2,}\",i)) , 0\n",
    "    \n",
    "def negatfeat(i):           # Negation for whole phrase\n",
    "    return len(re.findall(r\"(?i)(\\b(no|none|not|nothing|neither|never|can't|isn't|doesn't|wouldn't|shouldn't)\\b).*(,|\\.|:|;|!|\\?)\",i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashem = open(\"joyhashtag.txt\",\"r\")   \n",
    "hashdic = {}   # dictionary to keep hashtag emotions\n",
    "for i in hashem:\n",
    "    tem = list(map(str,i.split()))\n",
    "    hashdic[tem[1]] = float(tem[2])\n",
    "    \n",
    "wordem = open(\"wordemotion.txt\",\"r\")\n",
    "wordemodic = {}      # count words for particular emotion (anger or joy)\n",
    "for i in wordem:\n",
    "    tem = list(map(str,i.split()))\n",
    "    if tem[1] == \"joy\" and tem[2] == \"1\":\n",
    "        wordemodic[tem[0]] = 1\n",
    "        \n",
    "emotions = pandas.read_csv(\"emotion.csv\",sep = \"\\t\",error_bad_lines=False,)  \n",
    "#print(emotions)  # dictionary for emoticons aggregate\n",
    "emotiondic = {}\n",
    "for i in range(len(emotions[\"word\"])):\n",
    "    emotiondic[emotions[\"word\"][i]] = float(emotions[\"joy\"][i])\n",
    "#print(emotiondic)\n",
    "\n",
    "\n",
    "def featurehash(trainall,scoretrain):\n",
    "    for i in range(len(trainall)):\n",
    "        temp = list(map(str,trainall[i]))\n",
    "        sc = 0.0\n",
    "        for j in temp:\n",
    "            if j in hashfeat:\n",
    "                sc += hashfeat[j]\n",
    "        scoretrain[i].append(sc)\n",
    "\n",
    "        \n",
    "# Dictionary name define itself\n",
    "emoticondic = {}\n",
    "emoticon = open(\"emotican.txt\",\"r\")\n",
    "for i in emoticon:\n",
    "    tem = list(map(str,i.split()))\n",
    "    emoticondic[tem[0]] = float(tem[1])\n",
    "#print(emoticondic)\n",
    "\n",
    "sent140dic = {}\n",
    "sent140 = open(\"sent140.txt\",\"r\")\n",
    "for i in sent140:\n",
    "    tem = list(map(str,i.split()))\n",
    "    #print(tem)\n",
    "    sent140dic[tem[0]] = [float(tem[2]), float(tem[3])]\n",
    "    \n",
    "    \n",
    "hashsentdic = {}\n",
    "hashsent = open(\"hashsent.txt\",\"r\")\n",
    "for i in hashsent:\n",
    "    tem = list(map(str,i.split()))\n",
    "    #print(tem)\n",
    "    hashsentdic[tem[0]] = [float(tem[2]), float(tem[3])]\n",
    "    \n",
    "    \n",
    "mpqadic = {}\n",
    "mpqa = open(\"mpqa.txt\",\"r\")\n",
    "for i in mpqa:\n",
    "    tem = list(map(str,i.split()))\n",
    "    mpqadic[tem[0]] = tem[0]\n",
    "    \n",
    "bingliudic = {}\n",
    "bingliu = pandas.read_csv(\"bingliu.csv\",sep = \",\",error_bad_lines=False)\n",
    "#print(bingliu)\n",
    "for i in range(len(bingliu[\"word\"])):\n",
    "    bingliudic[bingliu[\"word\"][i]] = bingliu[\"emotion\"][i]\n",
    "#print(bingliudic)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess function preprocessses the data which is used to make training score matrix and test score matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(train,trainall,result_train,scoretrain):\n",
    "    print(\"Preprocessing start  ..........\\n\")\n",
    "    print(\"making Score matrix\")\n",
    "    for i in train:\n",
    "        scoretrain.append([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
    "        temp = list(map(str,i.split()))\n",
    "        #print(temp)\n",
    "        \n",
    "        result_train.append(float(temp.pop()))\n",
    "        temp.pop(0)\n",
    "        temp.pop()\n",
    "        for j in temp:\n",
    "            scoretrain[-1][0] += enlongated(j)\n",
    "            scoretrain[-1][1] +=hashtag(j)\n",
    "            scoretrain[-1][2] +=allcaps(j)\n",
    "            scoretrain[-1][3] +=taggedp(j)\n",
    "            scoretrain[-1][4] += negations(j)\n",
    "            scoretrain[-1][5] += wordemo(j,wordemodic)\n",
    "        scoretrain[-1][6] = hashemo(temp,hashdic)\n",
    "        scoretrain[-1][7] = emotionsfeat(temp,emotiondic)\n",
    "        \n",
    "        scoretrain[-1].append(emoticonfeat(temp,emoticondic))\n",
    "        aaa0,aaa1 = sent140feat(temp,sent140dic)\n",
    "        scoretrain[-1].append(aaa0)\n",
    "        scoretrain[-1].append(aaa1)\n",
    "        aaa0,aaa1 = hashsentfeat(temp,hashsentdic)\n",
    "        scoretrain[-1].append(aaa0)\n",
    "        scoretrain[-1].append(aaa1)\n",
    "        trainall.append(\" \".join(temp))\n",
    "        #aaa0,aaa1 = bingliufeat(temp,bingliudic)\n",
    "        #scoretrain[-1].append(aaa0)\n",
    "        #scoretrain[-1].append(aaa1)\n",
    "        #aaa1,aaa2 = mpqafeat(temp,mpqadic)\n",
    "        #ccoretrain[-1].append(aaa0)\n",
    "        #scoretrain[-1].append(aaa1)\n",
    "        \n",
    "        aaa0,aaa1 = puncfeat(i)\n",
    "        scoretrain[-1].append(aaa0)\n",
    "        scoretrain[-1].append(aaa1)\n",
    "        \n",
    "        scoretrain[-1].append(negatfeat(i))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    vader(scoretrain,trainall)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For printing statistics for particular model including Pearson and spearman coff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def statis(result_test,sss):\n",
    "    mae = metrics.mean_absolute_error(result_test, sss)    \n",
    "    mse = metrics.mean_squared_error(result_test, sss)     \n",
    "    rmse = np.sqrt(mse) #mse**(0.5)      # \n",
    "    r2 = metrics.r2_score(result_test, sss)\n",
    "\n",
    "    print(\"Results of sklearn.metrics:\")\n",
    "    print(\"MAE:\",mae)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"R-Squared:\", r2)\n",
    "    print(\"\\npearson corr. , p valve =\",stats.pearsonr(result_test,sss))\n",
    "    print(stats.spearmanr(result_test,sss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loading ..........\n",
      "\n",
      "Preprocessing start  ..........\n",
      "\n",
      "making Score matrix\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "hashfeat = {}    # DICTIONARY FOR hashtags predicting nature for sentence \n",
    "\n",
    "print(\"Training loading ..........\\n\")\n",
    "train = open(\"joy_train.txt\", \"r\")  # open file \n",
    "trainall = []  # contain training sentences\n",
    "result_train = []   # result for training\n",
    "scoretrain = []    # features for training data\n",
    "preprocess (train,trainall,result_train,scoretrain)   # preprocessing\n",
    "\n",
    "for i in range(len(trainall)):\n",
    "    temp = list(map(str,trainall[i]))\n",
    "    for j in temp:\n",
    "        if j[0] == \"#\":\n",
    "            if j in hashfeat:\n",
    "                if result_train[i] > 0.55:\n",
    "                    hashfeat[j] += 1\n",
    "                else:\n",
    "                    hashfeat[j] -= 1\n",
    "            else:\n",
    "                if result_train[i] > 0.55:\n",
    "                    hashfeat[j] = 1\n",
    "                else:\n",
    "                    hashfeat[j] = -1\n",
    "\n",
    "def featurehash(trainall,scoretrain):\n",
    "    for i in range(len(trainall)):\n",
    "        temp = list(map(str,trainall[i]))\n",
    "        sc = 0.0\n",
    "        for j in temp:\n",
    "            if j in hashfeat:\n",
    "                sc += hashfeat[j]\n",
    "        scoretrain[i].append(sc)\n",
    "featurehash(trainall,scoretrain)    # adding 13th feature\n",
    "\n",
    "#for i in trainall:\n",
    "print(len(scoretrain[0]))\n",
    "#    print(i)\n",
    "        \n",
    "        \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loading ..........\n",
      "\n",
      "Preprocessing start  ..........\n",
      "\n",
      "making Score matrix\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing loading ..........\\n\")\n",
    "\n",
    "test = open(\"joy_test.txt\", \"r\")\n",
    "testall = []  # testing sentences\n",
    "result_test = []  # test results\n",
    "scoretest  = []  # features for testing\n",
    "\n",
    "\n",
    "\n",
    "preprocess(test,testall,result_test,scoretest)\n",
    "featurehash(testall,scoretest)\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to numpy.......\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting to numpy.......\\n\")\n",
    "\n",
    "score_trainn = np.array(scoretrain)\n",
    "score_testn = np.array(scoretest)\n",
    "result_trainn = np.array(result_train) \n",
    "result_testn = np.array(result_test) # making numpy list\n",
    "\n",
    "#print(scoretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGRAMS (Predicting model using only ngram features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Model Uing only ngram (1,2) .......\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict Model Uing only ngram (1,2) .......\\n\")\n",
    "\n",
    "\n",
    "co_vect = CountVectorizer(ngram_range=(1,2))\n",
    "final_tr  = co_vect.fit_transform(trainall)\n",
    "final_te = co_vect.transform(testall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.15420231425785522\n",
      "MSE: 0.03564448169602555\n",
      "RMSE: 0.18879746210165418\n",
      "R-Squared: 0.24403539128098206\n",
      "\n",
      "pearson corr. , p valve = (0.5213387096617955, 5.2398190078086253e-51)\n",
      "SpearmanrResult(correlation=0.5180978734362142, pvalue=2.7262163258271837e-50)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVR Model\\n\")\n",
    "modelprid(SVR(),final_tr,final_te,result_trainn,result_test)    # Using SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of sklearn.metrics:\n",
      "MAE: 0.1491808847131856\n",
      "MSE: 0.034485458147149914\n",
      "RMSE: 0.18570260673224248\n",
      "R-Squared: 0.2686164973016536\n",
      "\n",
      "pearson corr. , p valve = (0.5423762096602273, 7.5268301320991715e-56)\n",
      "SpearmanrResult(correlation=0.543807341000414, pvalue=3.424538806360882e-56)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1,max_iter=5000) # arguments makes it very fast\n",
    "\n",
    "modelprid(clf,final_tr,final_te,result_trainn,result_test) # using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of sklearn.metrics:\n",
      "MAE: 0.1697700090384964\n",
      "MSE: 0.043848264394322295\n",
      "RMSE: 0.2093997717150673\n",
      "R-Squared: 0.07004578384547078\n",
      "\n",
      "pearson corr. , p valve = (0.31703182515356854, 3.908406542384875e-18)\n",
      "SpearmanrResult(correlation=0.31972121104362333, pvalue=1.966970236551013e-18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelprid(DecisionTreeRegressor(max_depth = 5),final_tr,final_te,result_trainn,result_test) # Using Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Predicting Model using 25 features \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\nPredicting Model using 25 features \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Models using our 25 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.1773791894677519\n",
      "MSE: 0.04607404126738206\n",
      "RMSE: 0.21464864608793147\n",
      "R-Squared: 0.022840481288747716\n",
      "\n",
      "pearson corr. , p valve = (0.19720127348638067, 1.081961636962667e-07)\n",
      "SpearmanrResult(correlation=0.18696891735770407, pvalue=4.863097010831192e-07)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVR Model\\n\")\n",
    "\n",
    "modelprid(SVR(),score_trainn,score_testn,result_trainn,result_test) # Using SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.1828443178235179\n",
      "MSE: 0.047419266119036595\n",
      "RMSE: 0.21775965218340287\n",
      "R-Squared: -0.005689667846048518\n",
      "\n",
      "pearson corr. , p valve = (nan, nan)\n",
      "SpearmanrResult(correlation=nan, pvalue=nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/usr/local/lib/python3.8/dist-packages/scipy/stats/stats.py:4196: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "print(\"MLP Model\\n\")\n",
    "\n",
    "modelprid(clf,score_trainn,score_testn,result_trainn,result_test)   # Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.16034970227136117\n",
      "MSE: 0.04157504706597592\n",
      "RMSE: 0.20389960045565544\n",
      "R-Squared: 0.1182572254596792\n",
      "\n",
      "pearson corr. , p valve = (0.4288740577786602, 2.5999101007991958e-33)\n",
      "SpearmanrResult(correlation=0.4452417264632753, pvalue=4.6162475416198666e-36)\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Model\\n\")\n",
    "\n",
    "modelprid(DecisionTreeRegressor(max_depth = 5),score_trainn,score_testn,result_trainn,result_test)  # Using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Predicting Model using N Gram + 25 features \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nPredicting Model using N Gram + 25 features \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting model by adding Ngrams and 25 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concating all features....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_matrix1 = final_tr.toarray()\n",
    "feature_matrix2 = final_te.toarray()      # Convert sparse to numpy\n",
    "\n",
    "print(\"Concating all features....\")\n",
    "score_trainnn = np.concatenate(( score_trainn, feature_matrix1), 1)  \n",
    "score_testnn = np.concatenate((score_testn,feature_matrix2), 1)      # adds Ngram numpy and 13 features matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.17757945908179387\n",
      "MSE: 0.04611190068032624\n",
      "RMSE: 0.21473681724456622\n",
      "R-Squared: 0.022037541396487503\n",
      "\n",
      "pearson corr. , p valve = (0.19532428923789066, 1.4341001747112075e-07)\n",
      "SpearmanrResult(correlation=0.18402663569007363, pvalue=7.380887921544649e-07)\n"
     ]
    }
   ],
   "source": [
    "print(\"SVR Model\\n\")\n",
    "\n",
    "modelprid(SVR(),score_trainnn,score_testnn,result_trainn,result_test)  # Using SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.1540048443607135\n",
      "MSE: 0.03810123651451493\n",
      "RMSE: 0.19519538036161338\n",
      "R-Squared: 0.1919314019197067\n",
      "\n",
      "pearson corr. , p valve = (0.4737423994261761, 3.181513337153329e-41)\n",
      "SpearmanrResult(correlation=0.46547613218087863, pvalue=1.122539034208128e-39)\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Model\\n\")\n",
    "\n",
    "modelprid(DecisionTreeRegressor(max_depth = 5),score_trainnn,score_testnn,result_trainn,result_test)  # Using Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Model\n",
      "\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.18271104774636623\n",
      "MSE: 0.04769172346481916\n",
      "RMSE: 0.21838434803075782\n",
      "R-Squared: -0.011468068905534423\n",
      "\n",
      "pearson corr. , p valve = (0.06350755270438262, 0.08994082043091499)\n",
      "SpearmanrResult(correlation=0.08517360495097508, pvalue=0.022842743576807167)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000) # arguments makes it very fast\n",
    "print(\"MLP Model\\n\")\n",
    "\n",
    "modelprid(clf,score_trainnn,score_testnn,result_trainn,result_test)  # using MLP\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cmdlinetips.com/2019/08/how-to-compute-pearson-and-spearman-correlation-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason for having nan values in Pearson and Spearman coff.  \n",
    "  \n",
    "  https://datascience.stackexchange.com/questions/10262/why-is-the-correlation-coefficient-of-a-constant-function-with-function-input-is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
