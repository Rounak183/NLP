{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow Instructions:\n",
    "+ Features Implemented can be seen in scoremat function.\n",
    "+ We implement all the features + Addtional features and we can include or exclude them by directly changing scoremat function.\n",
    "+ Program is optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from time import time\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from os import path\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn import metrics\n",
    "from sys import getsizeof\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPClassifier , MLPRegressor\n",
    "from scipy import stats\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlongated(st):   # elongated Feature\n",
    "    temp = re.sub(r'(.)\\1+', r'\\1\\1', st)\n",
    "    if len(temp) == len(st):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def hashtag(st):     # Count Hashtag\n",
    "    if st[0] == \"#\":\n",
    "        return 1\n",
    "    return 0 \n",
    "\n",
    "def allcaps(st):     # count all capital words\n",
    "    if st.isupper():\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def negations(st):    # to count negations(BONUS)\n",
    "    if st.lower() in [\"not\",\"nor\",\"neither\",\"no\",\"never\",\"nope\"]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def emoticonfeat(st,dic):  # Emoticon feature\n",
    "    aa = 0\n",
    "    for i in st:\n",
    "        if i in dic:\n",
    "            aa += dic[i]\n",
    "    return aa\n",
    "\n",
    "\n",
    "def sent140feat(st,dic):    # lexicon features\n",
    "    posi = []\n",
    "    neg = []\n",
    "    for i in st:\n",
    "        if i in dic:\n",
    "            posi.append(dic[i][0])\n",
    "            neg.append(dic[i][1])\n",
    "    if posi == []:\n",
    "        return 0,0\n",
    "    return sum(posi)/len(posi) , sum(neg)/len(neg)\n",
    "\n",
    "def hashsentfeat(st,dic):    # lexicon feature\n",
    "    posi = []\n",
    "    neg = []\n",
    "    for i in st:\n",
    "        if i in dic:\n",
    "            posi.append(dic[i][0])\n",
    "            neg.append(dic[i][1])\n",
    "    if posi == []:\n",
    "        return 0,0\n",
    "    return sum(posi)/len(posi) , sum(neg)/len(neg)\n",
    "\n",
    "def wordemofeat(st,dic):         # Counts Emotion word count:\n",
    "    aa = 0\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            aa += dic[i.lower()]\n",
    "    return aa\n",
    "\n",
    "\n",
    "def mpqafeat(st,dic):\n",
    "    posi = 0\n",
    "    neg = 0\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            if dic[i.lower()] == \"positive\":\n",
    "                posi += 1\n",
    "            elif dic[i.lower()] == \"negative\":\n",
    "                neg += 1\n",
    "    return posi, neg\n",
    "\n",
    "def bingliufeat(st,dic):\n",
    "    posi = 0\n",
    "    neg = 0\n",
    "    for i in st:\n",
    "        if i.lower() in dic:\n",
    "            #print(i.lower())\n",
    "            #print(dic[i.lower()])\n",
    "            if dic[i.lower()] == \"positive\":\n",
    "                posi += 1\n",
    "            elif dic[i.lower()] == \"negative\":\n",
    "                neg += 1\n",
    "    return posi, neg\n",
    "    \n",
    "def puncfeat(i):    # Punctuation feature\n",
    "    if i[-1] == \"!\" or i[-1]==\"?\":\n",
    "        return len(re.findall(\"[!?]{2,}\",i)) , 1\n",
    "    else:\n",
    "        return len(re.findall(\"[!?]{2,}\",i)) , 0\n",
    "    \n",
    "    \n",
    "def negatfeat(i):  # Negation feature\n",
    "    return len(re.findall(r\"(?i)(\\b(no|none|not|nothing|neither|never|can't|isn't|doesn't|wouldn't|shouldn't)\\b).*(,|\\.|:|;|!|\\?)\",i))    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def tagger(tag,count):   \n",
    "    if tag in count:\n",
    "        return count[tag]\n",
    "    return 0\n",
    "    \n",
    "\n",
    "def POS(feature,i):  # POS feature\n",
    "    tags = nltk.pos_tag(i.split())\n",
    "    counts = Counter( tag for word,  tag in tags)\n",
    "    feature.append(tagger(\"NN\",counts))\n",
    "    feature.append(tagger(\"DT\",counts))\n",
    "    feature.append(tagger(\"VBZ\",counts))\n",
    "    feature.append(tagger(\"JJ\",counts))\n",
    "    feature.append(tagger(\"RB\",counts))\n",
    "    feature.append(tagger(\"JJ\",counts))\n",
    "    feature.append(tagger(\"CC\",counts))\n",
    "    feature.append(tagger(\"PRP\",counts))\n",
    "    feature.append(tagger(\"TO\",counts))\n",
    "    feature.append(tagger(\"IN\",counts))\n",
    "    feature.append(tagger(\"VB\",counts))\n",
    "    feature.append(tagger(\"NNP\",counts))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #print(accu)\n",
    "    #print(len(accu))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions For stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statsnaive(result_test,pred_test):\n",
    "    accu = []\n",
    "    for i in range(len(result_test)):\n",
    "        if result_test[i] == pred_test[i]:\n",
    "            accu.append(1)\n",
    "        else:\n",
    "            accu.append(0)\n",
    "    print(\"accuracy = \", sum(accu) * 100 / len(accu))\n",
    "\n",
    "def statis(test,pred):   # FInal function for statistics\n",
    "    \n",
    "    statsnaive(test,pred)\n",
    "    print(\"0 represents negative sentiment and 1 represents positive sentiment\")\n",
    "    confu_matrix = [[0,0],[0,0]]\n",
    "    for i in range(len(test)):\n",
    "        if test[i] == 4 and pred[i] == 4:\n",
    "            confu_matrix[1][1] += 1\n",
    "        if test[i] == 4 and pred[i] == 0:\n",
    "            confu_matrix[1][0] += 1\n",
    "        if test[i] == 0 and pred[i] == 4:\n",
    "            confu_matrix[0][1] += 1\n",
    "        if test[i] == 0 and pred[i] == 0:\n",
    "            confu_matrix[0][0] += 1\n",
    "    print(\"Confusion matrix (row represents original result and col represents predicted results :-\")\n",
    "    print(\"   0     1\")\n",
    "    print(0,*confu_matrix[0])\n",
    "    print(1,*confu_matrix[1])\n",
    "    \n",
    "    tp0 = confu_matrix[0][0]\n",
    "    fp0 = confu_matrix[1][0]\n",
    "    fn0 = confu_matrix[0][1]\n",
    "    \n",
    "    tp1 = confu_matrix[1][1]\n",
    "    fp1 = confu_matrix[0][1]\n",
    "    fn1 = confu_matrix[1][0]\n",
    "    \n",
    "    print(\"\\nFor negative sentiment:\")\n",
    "    prec0 = tp0 / (tp0 + fp0)\n",
    "    recall0 = tp0 / (tp0 + fn0)\n",
    "    f1s0 = 2 * (prec0 * recall0) / (prec0 + recall0)\n",
    "    print(\"precsion =\",prec0)\n",
    "    print(\"recall = \",recall0)\n",
    "    print(\"f1s = \",f1s0)\n",
    "    \n",
    "    \n",
    "    print(\"\\nFor positive sentiment:\")\n",
    "    prec1 = tp1 / (tp1 + fp1)\n",
    "    recall1 = tp1 / (tp1 + fn1)\n",
    "    f1s1 = 2 * (prec1 * recall1) / (prec1 + recall1) \n",
    "    print(\"precsion =\",prec1)\n",
    "    print(\"recall = \",recall1)\n",
    "    print(\"f1s = \",f1s1)\n",
    "    \n",
    "    print(\"Aver. Precision = \",(prec0 +prec1)/2)\n",
    "    print(\"Aver. recall = \",(recall0 + recall1)/2)\n",
    "    print(\"Aver. f1s = \", (f1s0 + f1s1)/2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries for Features containing respective values\n",
    "Used in scoremat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticondic = {}        \n",
    "emoticon = open(\"emotican.txt\",\"r\")\n",
    "for i in emoticon:\n",
    "    tem = list(map(str,i.split()))\n",
    "    emoticondic[tem[0]] = float(tem[1])\n",
    "#print(emoticondic)\n",
    "\n",
    "\n",
    "sent140dic = {}\n",
    "sent140 = open(\"sent140.txt\",\"r\")\n",
    "for i in sent140:\n",
    "    tem = list(map(str,i.split()))\n",
    "    #print(tem)\n",
    "    sent140dic[tem[0]] = [float(tem[2]), float(tem[3])]\n",
    "    \n",
    "    \n",
    "hashsentdic = {}\n",
    "hashsent = open(\"hashsent.txt\",\"r\")\n",
    "for i in hashsent:\n",
    "    tem = list(map(str,i.split()))\n",
    "    #print(tem)\n",
    "    hashsentdic[tem[0]] = [float(tem[2]), float(tem[3])]\n",
    "    \n",
    "    \n",
    "wordem = open(\"wordemotion.txt\",\"r\")\n",
    "wordemodic = {}      # count words for particular emotion (anger or joy)\n",
    "for i in wordem:\n",
    "    tem = list(map(str,i.split()))\n",
    "    if tem[1] == \"joy\" and tem[2] == \"1\":\n",
    "        wordemodic[tem[0]] = 1\n",
    "        \n",
    "        \n",
    "    \n",
    "mpqadic = {}\n",
    "mpqa = open(\"mpqa.txt\",\"r\")\n",
    "for i in mpqa:\n",
    "    tem = list(map(str,i.split()))\n",
    "    mpqadic[tem[0]] = tem[0]\n",
    "    \n",
    "bingliudic = {}\n",
    "bingliu = pandas.read_csv(\"bingliu.csv\",sep = \",\",error_bad_lines=False)\n",
    "#print(bingliu)\n",
    "for i in range(len(bingliu[\"word\"])):\n",
    "    bingliudic[bingliu[\"word\"][i]] = bingliu[\"emotion\"][i]\n",
    "#print(bingliudic)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing to made feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoremat(scoretrain,trainfile,probyes):\n",
    "    senten = trainfile[\"5\"].tolist()\n",
    "    result_train = trainfile[\"0\"].tolist()\n",
    "    for i in senten:\n",
    "        temp = list(map(str,i.split()))\n",
    "        nv_posi,nv_neg = naiveb(temp,probyes)\n",
    "        scoretrain.append([nv_posi,nv_neg,0,0,0,0])\n",
    "        st = list(map(str,i[1].split()))\n",
    "        for j in st:\n",
    "            scoretrain[-1][2] += enlongated(j)\n",
    "            scoretrain[-1][3] += hashtag(j)\n",
    "            scoretrain[-1][4] += allcaps(j)\n",
    "            scoretrain[-1][5] += negations(j)\n",
    "        scoretrain[-1].append(emoticonfeat(temp,emoticondic))\n",
    " \n",
    "        #aaa0,aaa1 = sent140feat(temp,sent140dic)   # significant change in accuracy and time\n",
    "        #scoretrain[-1].append(aaa0)\n",
    "        #scoretrain[-1].append(aaa1)\n",
    "        #aaa0,aaa1 = hashsentfeat(temp,hashsentdic)  # significant change in accuracy and time\n",
    "        #scoretrain[-1].append(aaa0)\n",
    "        #scoretrain[-1].append(aaa1)\n",
    "        \n",
    "        \n",
    "        #trainall.append(\" \".join(temp))\n",
    "        \n",
    "        aaa0,aaa1 = bingliufeat(temp,bingliudic)\n",
    "        scoretrain[-1].append(aaa0)\n",
    "        scoretrain[-1].append(aaa1)\n",
    "        aaa1,aaa2 = mpqafeat(temp,mpqadic)\n",
    "        scoretrain[-1].append(aaa0)\n",
    "        scoretrain[-1].append(aaa1)\n",
    "        \n",
    "        aaa0,aaa1 = puncfeat(i)\n",
    "        scoretrain[-1].append(aaa0)\n",
    "        scoretrain[-1].append(aaa1)\n",
    "        \n",
    "        scoretrain[-1].append(negatfeat(i))\n",
    "        POS(scoretrain[-1],i.lower())\n",
    "        #print(len(scoretrain))\n",
    "        \n",
    "        #scoretrain[-1].append(wordemofeat(temp,wordemodic)) # reduce 2 -3 percent accuracy\n",
    "        \n",
    "        \n",
    "    return scoretrain,result_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def naiveb(st,probyes):     # naive bayes\n",
    "    scp = 1 \n",
    "    scn = 1\n",
    "    probno = 1-probyes\n",
    "    #print(st)\n",
    "    for i in st:\n",
    "        if i in dictword:\n",
    "            scp*= dictword[i][1]/probyes\n",
    "            scn *= dictword[i][0]/probno\n",
    "    return scp / (scp + scn) , scn/(scp+scn)\n",
    "\n",
    "\n",
    "def naivebres(scoretest):\n",
    "    pred_test = []\n",
    "    for i in scoretest:\n",
    "        if i[0] > i[1]:\n",
    "            pred_test.append(4)\n",
    "        else:\n",
    "            pred_test.append(0)\n",
    "        #print(i[0],i[1])\n",
    "    return pred_test\n",
    "\n",
    "            \n",
    "def statsnaive(result_test,pred_test):\n",
    "    accu = []\n",
    "    for i in range(len(result_test)):\n",
    "        if result_test[i] == pred_test[i]:\n",
    "            accu.append(1)\n",
    "        else:\n",
    "            accu.append(0)\n",
    "    print(\"accuracy = \", sum(accu) * 100 / len(accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code Starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bag of words.....\n"
     ]
    }
   ],
   "source": [
    "smoothing = 1    # Take smoothing for dictionary as 1\n",
    "start_time = time()   \n",
    "\n",
    "scoretrain = []  # scoring matrix (features)\n",
    "result_train = [] \n",
    "\n",
    "trainfile = pandas.read_csv('strainfull.csv', sep=',')\n",
    "result_train = trainfile[\"0\"].tolist()\n",
    "\n",
    "probyes = (sum(result_train) / 4 ) / len(result_train)\n",
    "\n",
    "\n",
    "print(\"Making bag of words.....\")\n",
    "if path.isfile('ser_dict.txt'):\n",
    "    dictword = pickle.load(open(\"ser_dict.txt\" ,\"rb\"))\n",
    "else:\n",
    "    dictword = {}      # Bag of Words \n",
    "    for ttt in range(len(trainfile[\"5\"])):\n",
    "        if ttt%100000 == 0:\n",
    "            print(ttt)\n",
    "        i = [trainfile[\"0\"][ttt],trainfile[\"5\"][ttt]]\n",
    "        st = list(map(str,i[1].split()))\n",
    "        #print(st)\n",
    "        for j in st:\n",
    "            if j in dictword:\n",
    "                if i[0] == 4:\n",
    "                    dictword[j][1] += 1\n",
    "                else:\n",
    "                    dictword[j][0] += 1\n",
    "            else:\n",
    "                dictword[j] = [smoothing,smoothing]\n",
    "                if i[0] == 4:\n",
    "                    dictword[j][1] += 1\n",
    "                else:\n",
    "                    dictword[j][0] += 1\n",
    "    print(len(dictword))\n",
    "    with open('ser_dict.txt', 'wb') as fh:\n",
    "        pickle.dump(dictword,fh)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_vect = CountVectorizer(ngram_range=(1,2))  # preparing ngram count_vectorizer dictionary\n",
    "co_vect.fit(trainfile[\"5\"][:300000]) # cacn be increased or decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loading........\n"
     ]
    }
   ],
   "source": [
    "trainfile = pandas.read_csv('strain.csv', sep=',')   \n",
    "\n",
    "print(\"Train loading........\")\n",
    "scoretrain , result_train =  scoremat(scoretrain,trainfile,probyes)\n",
    "trainall = trainfile[\"5\"].tolist()# store training sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loading\n",
      "Prepare Scoring matrix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scoretest = []   # scoring matrix\n",
    "result_test = []\n",
    "\n",
    "print(\"testing loading\")\n",
    "testfile = pandas.read_csv('stest.csv', sep=',')\n",
    "testfile = testfile[:1500]\n",
    "testall = testfile[\"5\"].tolist()\n",
    "\n",
    "print(\"Prepare Scoring matrix\")\n",
    "scoretest ,result_test = scoremat(scoretest,testfile,probyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Naive Bayes..... \n",
      "\n",
      "accuracy =  75.13333333333334\n",
      "0 represents negative sentiment and 1 represents positive sentiment\n",
      "Confusion matrix (row represents original result and col represents predicted results :-\n",
      "   0     1\n",
      "0 645 81\n",
      "1 292 482\n",
      "\n",
      "For negative sentiment:\n",
      "precsion = 0.688367129135539\n",
      "recall =  0.8884297520661157\n",
      "f1s =  0.7757065544197236\n",
      "\n",
      "For positive sentiment:\n",
      "precsion = 0.8561278863232682\n",
      "recall =  0.6227390180878553\n",
      "f1s =  0.7210172026925955\n",
      "Aver. Precision =  0.7722475077294035\n",
      "Aver. recall =  0.7555843850769854\n",
      "Aver. f1s =  0.7483618785561595\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for Naive Bayes..... \\n\")\n",
    "pred_test = naivebres(scoretest)   # Naive Bayes \n",
    "statis(result_test,pred_test)      # Stats for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoretrain = np.array(scoretrain)\n",
    "scoretest = np.array(scoretest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinate feature matrix with Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tr  = co_vect.fit_transform(trainall)\n",
    "feature_matrix1 = final_tr.toarray()\n",
    "score_train = np.concatenate((scoretrain, feature_matrix1), 1)\n",
    "\n",
    "model1 = SVC()\n",
    "model2 = DecisionTreeClassifier(max_depth = 5)\n",
    "model3 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1,max_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1..........\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1..........\")\n",
    "if path.isfile('model1.txt'):\n",
    "    model1 = pickle.load(open(\"model1.txt\" ,\"rb\"))\n",
    "else:\n",
    "    with open('model1.txt', 'wb') as fh:\n",
    "        model1.fit(score_train, result_train)\n",
    "        pickle.dump(model1,fh)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2..........\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Model 2..........\")\n",
    "if path.isfile('model2.txt'):\n",
    "    model2 = pickle.load(open(\"model2.txt\" ,\"rb\"))\n",
    "else:\n",
    "    with open('model2.txt', 'wb') as fh:\n",
    "        model2.fit(score_train, result_train)\n",
    "        pickle.dump(model2,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3..........\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 3..........\")\n",
    "if path.isfile('model3.txt'):\n",
    "    model3 = pickle.load(open(\"model3.txt\" ,\"rb\"))\n",
    "else:\n",
    "    with open('model3.txt', 'wb') as fh:\n",
    "        model3.fit(score_train, result_train)\n",
    "        pickle.dump(model3,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are predicting our model by spliting test data into 6000 size's slits\n",
    "It reduces space complexity \n",
    "Decreases time\n",
    "No change in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on  0 to 1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = []\n",
    "start_time = time()\n",
    "gapp = 1500  # Breaking test splits \n",
    "svc = []  # svc results\n",
    "dt = []   # dt results\n",
    "mlp = []   # mlp results\n",
    "\n",
    "for i in range(0,len(scoretest),gapp):\n",
    "    print(\"Testing on \",i ,\"to\", i + gapp)\n",
    "    result_testl = result_test[i:i+gapp]\n",
    "    final_te = co_vect.transform(testall[i:i+gapp])\n",
    "    feature_matrix = final_te.toarray() \n",
    "    score_testnn = np.concatenate((scoretest[i:i+gapp],feature_matrix),1)  # Concating All features\n",
    "    svc += list(model1.predict(score_testnn))             # SVC Results\n",
    "    dt +=  list(model2.predict(score_testnn))            # Decision Tree Results\n",
    "    mlp += list(model3.predict(score_testnn))              # For MLP  \n",
    "    #print(len(dt))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Stats For SVC Model\n",
      "accuracy =  77.0\n",
      "0 represents negative sentiment and 1 represents positive sentiment\n",
      "Confusion matrix (row represents original result and col represents predicted results :-\n",
      "   0     1\n",
      "0 624 102\n",
      "1 243 531\n",
      "\n",
      "For negative sentiment:\n",
      "precsion = 0.7197231833910035\n",
      "recall =  0.859504132231405\n",
      "f1s =  0.783427495291902\n",
      "\n",
      "For positive sentiment:\n",
      "precsion = 0.8388625592417062\n",
      "recall =  0.686046511627907\n",
      "f1s =  0.7547974413646056\n",
      "Aver. Precision =  0.7792928713163548\n",
      "Aver. recall =  0.772775321929656\n",
      "Aver. f1s =  0.7691124683282538\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\\nStats For SVC Model\")\n",
    "statis(result_test,svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Stats For Decision Tree Model\n",
      "accuracy =  75.53333333333333\n",
      "0 represents negative sentiment and 1 represents positive sentiment\n",
      "Confusion matrix (row represents original result and col represents predicted results :-\n",
      "   0     1\n",
      "0 601 125\n",
      "1 242 532\n",
      "\n",
      "For negative sentiment:\n",
      "precsion = 0.7129300118623962\n",
      "recall =  0.8278236914600551\n",
      "f1s =  0.7660930528999362\n",
      "\n",
      "For positive sentiment:\n",
      "precsion = 0.8097412480974124\n",
      "recall =  0.6873385012919897\n",
      "f1s =  0.7435359888190077\n",
      "Aver. Precision =  0.7613356299799043\n",
      "Aver. recall =  0.7575810963760223\n",
      "Aver. f1s =  0.754814520859472\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n\\nStats For Decision Tree Model\")\n",
    "statis(result_test,dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Stats For MLP Model\n",
      "accuracy =  75.66666666666667\n",
      "0 represents negative sentiment and 1 represents positive sentiment\n",
      "Confusion matrix (row represents original result and col represents predicted results :-\n",
      "   0     1\n",
      "0 595 131\n",
      "1 234 540\n",
      "\n",
      "For negative sentiment:\n",
      "precsion = 0.7177322074788902\n",
      "recall =  0.8195592286501377\n",
      "f1s =  0.7652733118971061\n",
      "\n",
      "For positive sentiment:\n",
      "precsion = 0.8047690014903129\n",
      "recall =  0.6976744186046512\n",
      "f1s =  0.7474048442906575\n",
      "Aver. Precision =  0.7612506044846016\n",
      "Aver. recall =  0.7586168236273945\n",
      "Aver. f1s =  0.7563390780938818\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\\nStats For MLP Model\")\n",
    "statis(result_test,mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
